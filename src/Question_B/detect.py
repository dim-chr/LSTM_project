# -*- coding: utf-8 -*-
"""detect.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RlgLG_GZusb-r9LTlwcY_Xh7aXqXbchZ
"""

import os
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
from pandas.plotting import register_matplotlib_converters
from matplotlib import rcParams
import numpy as np
import seaborn as sns
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import RepeatVector
from keras.layers import TimeDistributed
from sklearn.preprocessing import StandardScaler

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

# register_matplotlib_converters()
sns.set(style='whitegrid', palette='muted', font_scale=1.5)

rcParams['figure.figsize'] = 22, 10

# RANDOM_SEED = 42

# np.random.seed(RANDOM_SEED)
# tf.random.set_seed(RANDOM_SEED)

num_time_series = 1
# csv_path = os.path.join(os.path.abspath(__file__), "../../../dir/nasdaq2007_17.csv")
csv_path = "/content/nasdaq2007_17.csv"  #! Only for google colab

df = pd.read_csv(csv_path, header=None, delimiter='\t')
file_ids = df.iloc[:, [0]].values
df = df.drop(df.columns[0], axis=1)
df = df.transpose()
print("Number of rows and columns:", df.shape)

TIME_STEPS = 30

# Training data: 95%, Test data: 5%
train_size = int(len(df) * 0.95)
test_size = len(df) - train_size
print("Train size: ", train_size)

scaler = StandardScaler()

# Creating a data structure with 30 time-steps and 1 output
X_train = []
y_train = []
for step in range (0, num_time_series):
    
    training_set = df.iloc[:train_size, [step]].values
    training_set_scaled = scaler.fit_transform(training_set)

    for i in range(TIME_STEPS, train_size):
        X_train.append(training_set_scaled[i-TIME_STEPS:i, 0])
        y_train.append(training_set_scaled[i, 0])

X_train, y_train = np.array(X_train), np.array(y_train)
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
print(X_train.shape)

model = Sequential()

model.add(LSTM(units=64, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dropout(rate=0.2))

model.add(RepeatVector(n=X_train.shape[1]))

model.add(LSTM(units=64, return_sequences=True))
model.add(Dropout(rate=0.2))

model.add(TimeDistributed(Dense(units=X_train.shape[2])))

model.compile(loss='mae', optimizer='adam')

history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.1, shuffle=False)

plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
fig = plt.gcf()
fig.set_size_inches(18.5, 10.5)
plt.show()
plt.clf()

X_train_pred = model.predict(X_train)
train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)
print(np.max(train_mae_loss))

sns.distplot(train_mae_loss, bins=50, kde=True)

dataset_train = df.iloc[:train_size, [0]]
dataset_test = df.iloc[train_size:, [0]]
print(len(dataset_test))
dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)

inputs = dataset_total[len(dataset_total) - len(dataset_test) - TIME_STEPS:].values
inputs = inputs.reshape(-1,1)
inputs = scaler.transform(inputs)
X_test = []
for i in range(TIME_STEPS, test_size+TIME_STEPS):
    X_test.append(inputs[i-TIME_STEPS:i, 0])

X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

X_test_pred = model.predict(X_test)
test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)
print(len(test_mae_loss))

sns.distplot(test_mae_loss, bins=50, kde=True)

THRESHOLD = np.max(train_mae_loss)
print(np.max(train_mae_loss))
print(np.max(test_mae_loss))
if THRESHOLD > np.max(test_mae_loss):
    THRESHOLD = np.max(test_mae_loss) - 0.05

# DataFrame containing the loss and the anomalies (values above the threshold)
test_score_df = pd.DataFrame(index=dataset_test[0:].index)
test_score_df['loss'] = test_mae_loss
test_score_df['threshold'] = THRESHOLD
test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold
test_score_df['close'] = dataset_test[0:]

plt.plot(test_score_df.index, test_score_df.loss, label='loss')
plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')
plt.xticks(rotation=25)
plt.legend()
fig = plt.gcf()
fig.set_size_inches(18.5, 10.5)
plt.show()
plt.clf()

anomalies = test_score_df[test_score_df.anomaly == True]

plt.plot( 
    dataset_test[TIME_STEPS:].index, 
    dataset_test[TIME_STEPS:],
    label='close price')

sns.scatterplot(
    anomalies.index, 
    anomalies.close, 
    color=sns.color_palette()[3], 
    s=52, 
    label='anomaly')

plt.xticks(rotation=25)
plt.legend()
fig = plt.gcf()
fig.set_size_inches(18.5, 10.5)
plt.show()
plt.clf()